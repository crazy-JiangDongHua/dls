这一章介绍了深度学习库的抽象，也就是如何设计深度学习库的接口。



深度学习库（抽象）的演化

1. caffe

   caffe采用的层的概念，需要为层设计前向传播和反向传播。这是一种非常自然的设计，因为神经网络本身就是用层堆叠的，但是这样做也有一些缺点：

   1. 反向传播是直接原地反向进行的，这样很难获得更高阶的导数。
   2. 对于层而言包含了前向计算和反向计算，也就是自动微分和组合模型这两个抽象都放在层中实现。当需要组合成一些复杂的模型是，设计反向计算将变得困难。

   为了修补这两个缺点，计算图的概念被提出了。模型的计算被看作是一个计算图，由计算图来处理自动微分。这样做将自动微分和组合模型这两个抽象分开了，自动微分在计算图（或者说Tensor层级）实现了，然后用更高级的类（例如nn.module）去组合模型。同时因为反向传播可以看作是在原来的计算图上拓展一部分计算图，因此可以在新的计算图上反向传播获得更高阶的梯度。

2. tensorflow 1.0

   tensorflow 1.0采用了静态图和声明式编程（现在tensorflow也开始拥抱动态图和命令式编程，但是版本兼容实在太差）。首先声明计算图，在做实际运算。这样做的好处是，计算时已经拥有了完整的计算图，这就很方便做优化。这样做的坏处就是，对用户不那么友好，例如定义控制流变得很麻烦，想要调试也不方便。

3. pytorch

   pytorch采用了动态图和命令式编程。每执行一步就构建一点计算图，同时马上计算出结果，这也被称作eager模式。这样做的好处是，对用户友好，可以使用python原生定义控制流，调试很方便。这样做的坏处就是计算时还不知道完整的计算图，很难去做优化。

   所以静态图和动态图更像是一种效率和方便的权衡。



之前的作业已经实现了动态图，用来组合模型的更高阶的接口（或者说抽象）有：

1. DataLoader和Preparation：加载数据，并对数据做预处理（包括数据增强）
2. Module：递归式地定义模块，构造模型。功能包括：
   1. 输入Tensor，输出Tensor
   2. 维护参数
   3. 对参数初始化
3. Initialization：包含一系列对参数初始化的方法
4. Loss：可以看作一种特殊的Module，一般输入Tensor，输出Scalar
5. Optimizer：包含一些列优化参数的方法