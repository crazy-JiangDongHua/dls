这一章简单介绍了训练模型时节省内存的技术，然后入门了模型的并行训练。



## 1. 模型训练节省内存的技术

1. 推理：推理是只需要保存所有权重，和一个中间变量
2. 训练：简单的方法需要保存中间变量（反向传播所需要的）和所有权重；以时间换空间的方法，将神将网络分成k段，每一段只保存初始节点的中间变量，反向传播时再次前向传播，获取所需要的中间变量，这样我们需要存储的中间变量就是 $O(n/k+k)$，当 $k=\sqrt{n}$ 时取最小。



## 2. 模型并行训练

1. 模型并行：模型太大，分段放在多个设备上，很难并行，需要使用类似流水线的技术并行

2. 数据并行：模型存放在每个设备上，分别处理一部分数据，然后将梯度汇总，再更新所有模型

   